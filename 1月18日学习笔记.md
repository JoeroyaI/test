# 1月18日吴恩达老师课程

①如果我们怀疑神经网络过度拟合了数据，即存在高方差问题，那么先想到的方法可能是正则化。另外一个解决高方差的方法就是准备更多数据，这也是非常可靠的方法，但是我们可能无法时时准备足够多的训练数据或者获取更多数据的成本很高。但是正则化通常有助于避免过度拟合或减少网络误差，接下来我们就来讲讲正则化的原理。

我们用logistic回归来实现这些设想，将成本函数J的最小值，也就是我们定义的成本函数，参数中包含一些训练数据和不同数据中个体预测的损失。w和b是逻辑回归的两个参数。w是一个多维度参数矢量，b是一个实数。在逻辑回归函数中加入正则化，只需要添加参数$$\lambda$$，也就是正则化参数，公式如下：

![](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610935675885.png)



这个方法称为L2正则化，因为这里用了欧几里得法线，所以$$||w||$$被称为向量参数w的L2范数。那为什么只正则化参数w，而不再加上参数b？因为w通常是一个高维参数矢量，已经可以表达高偏差问题。w可能含有很多参数，我们不可能拟合所有参数。而b只是单个数字。所以w几乎涵盖所有参数，而不是b。如果加了参数b，其实也没有什么太大影响，因为b只是众多参数中的一个，所以通常可以省略不计。

L2正则化时最常见的正则化类型，另外还有一个L1正则化：![](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610935725493.png)

上面被称为参数w向量的L1范数，无论分母是m还是2m，他都是一个比例常量。如果用的是L1正则化，那么w最终会是稀疏的，也就是说w向量中有很多0。虽然L1正则化使模型变得稀疏，却没有降低太多存储内存。

人们在训练网络时，越来越倾向于使用L2正则化。$$\lambda$$是正则化参数，我们通常使用验证集或交叉验证来配置这个参数，尝试各种各样的数据，寻找最好的参数。我们要考虑训练集之间的权衡，把参数正常值设置为较小值，这样可以避免过拟合。$$\lambda$$是另外一个需要调整的超级参数。

在Python变成语言中，$$\lambda$$是一个保留字段，编写代码时，我们删掉a，写成lambd，以免与Python中的保留字段冲突。上面就是在逻辑回归函数中实现L2正则化的过程。

如何在神经网络中实现L2正则化呢？神经网络含有一个成本函数，该函数中包含所有参数，字母L是神经网络的所含的层数：![1610937024563](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610937024563.png)

如何使用上面的范数实现梯度下降呢？首先用backprop计算出dw的值，backprop会给出j对$$w^{[l]}$$的偏导数，再把$$w^{[l]}$$替换为$$w^{[l]}$$-lr*d。现在我们既然已经增加了这个正则化项，现在我们给dw加上这一项，之后再计算这个更新项。![1610938144744](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610938144744.png)

使用新定义的$$dw^{[l]}$$，他的定义含有代价函数导数和相关参数以及最后添加的额外正则项。![1610938265292](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610938265292.png)

这也是L2正则化有时被称为“权重衰减”的原因，将上面的两个式子联立，得到：![1610938440909](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610938440909.png)

该正则项说明，不管$$w^{[l]}$$是什么，我们都试图让他变得更小。他有点像一般的梯度下降，只不过系数不太一样。

***

②为什么正则化可以有效预防过拟合呢？为什么他可以减少方差问题？我们通过两个例子来直观体会一下。我们假设图中是一个过拟合的神经网络。将代价函数的公式中添加正则项，他可以避免数据权值矩阵过大。那么为什么压缩L2范数可以减少过拟合呢？直观上理解就是如果正则化参数$$\lambda$$设置得足够大，权重矩阵w就被设置为接近0的值，直观理解就是把多隐藏单元的权重设为0，于是基本消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络就会变成一个很小的神经网络，小到如同一个逻辑回归单元 ，可是深度却很大。他会使网络从这个过拟合的状态更接近于左图的高偏差状态，但是$$lambda$$会存在一个中间值，于是会有一个接近正确值的中间状态。![1610940207989](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610940207989.png)

直观上理解就是$$\lambda$$增加到足够大，w会接近0，实际上是不会发生这种情况的。我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单。这个神经网络越来越接近于logistic回归。我们直觉上认为大量隐藏单元被消除了，其实不然，实际上该网络的所有隐藏单元依然存在，但是他们的影响变得更小了。神经网络变得简单了，貌似不会出现过拟合。因此我不确定这样的直觉经验是否有用，不过在编程执行正则化时，我们实际上会看到一些方差减少的结果。

我们再来直观的感受一下，正则化为什么可以预防过拟合？假设我们用的是这样的双曲激活函数，那么我们会发现，只要z非常小。如果z只涉及到少量参数，这里我们利用了双曲正切函数的线性状态。只要z可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。我们写出z的公式：
$$
z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}
$$
如果w很小，那么z也会很小。特别是，如果z的值最终在这个范围内，都是相对较小的值，那么g(z)大致呈线性。![1610940647079](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610940647079.png)

此时每层几乎都是线性的，和线性回归函数一样。第一节课我们讲过，如果每层都是线性的，那么整个网络就是一个线性网络，即使是一个非常深的深层网络，因为具有线性激活函数的特征，最终我们只能计算线性函数。因此他不适用于非常复杂的决策，以及过度拟合数据集合的非线性决策边界，如同我们之前看到过的过度拟合高方差的情况。![1610940893664](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610940893664.png)

总结一下，如果正则化参数变得很大，那么参数w就会很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上z的取值范围很小，这个激活函数，也就是曲线函数会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极其复杂的非线性函数，不会发生过拟合。

大家在编程作业实现正则化的时候，会亲眼看到这些结果。总结正规化之前，我给大家一个执行方面的小建议，在增加正则化项时，应用之前定义的代价函数J，我们做过修改，增加了一项，目的是预防权重过大。![1610941161087](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610941161087.png)

如果我们使用的是梯度下降函数，在调试梯度下降时，其中一步就是把代价函数J设计成这样一个函数，他代表梯度下降的调幅数量。

可以看到，代价函数对于每个梯度下降的每个调幅都单调递减。如果你实施的是正规化函数，请牢记J已经有一个全新的定义。如果你用的是一般的J函数，没有后面的那一项，你可能看不到单调递减现象。为了调试梯度下降，请务必使用新定义的J函数，他包含第二个正则化项，否则J可能不会在所有调幅范围内都单调递减。![1610941451037](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610941451037.png)

L2正则化是我在训练深度学习模型时最常用的一种方法。在深度学习中，还有一种方法也用到了正则化，就是dropout正则化，之后我们会学习。

***

③除了L2正则化，还有一个非常实用的正则化方法，就是dropout。我们来看看他的工作原理。假设我们训练下图中的神经网络，它存在过拟合，这就是dropout所要处理的。我们复制这个神经网络，dropout会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络的每一层，每一个节点都以抛硬币的方式设置概率。每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删掉从该点进出的连线。![1610943058870](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610943058870.png)

![1610943068782](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610943068782.png)

最后得到一个节点更少、规模更小的网络，然后用bp算法进行训练，这是网络节点精简后的一个样本。对于其他样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其他类型的节点集合。对于每个训练样本，我们都采用一个精简后的神经网络来训练他。这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可他真的有效。不过可想而知，我们针对每个样本训练规模极小的网络，最后我们可能会认识到为什么要正则化网络，因为我们在训练极小的网络。

如何实施dropout呢？方法有几种，接下来我讲的是一种最常用的方法，即inverted dropout。出于完整性考虑，我们用一个三层的网络来举例说明，编码中会有很多涉及到3的地方，我只举例说明如何在某一层中实施dropout。

首先要定义向量d，$$d^3$$表示一个三层的dropout向量，看他是否小于某数，我们称之为是keep-prob。keep-prob是一个具体数字，上个示例中他是0.5，本例中我们设置为0.8，他表示保留某个隐藏单元的概率。也就意味着消除任意一个隐藏单元的概率是0.2。$$d^3$$是一个矩阵，每个样本和每个隐藏单元，其在$$d^3$$中的对应值为1的概率都是0.8，其对应值为0的概率为0.2。随机数字小于0.8，他等于1的概率是0.8，等于0的概率是0.2。

接下来要做的就是从第三层中获取激活函数，这里我们叫他$$a^3$$，他含有要计算的激活函数，它的作用是过滤$$d^3$$中所有等于0的元素，而各个元素等于0的概率只有0.2，乘法运算最终把$$d^3$$中相应元素归零。![1610944005902](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610944005902.png)

如果用python实现该算法的话，$$d^3$$则是一个布尔型数组，值为true和false，而不是1和0。乘法运算依然有效，python会把true和false翻译为1和0。大家可以用python去尝试一下。

最后我们向外扩展$$a^3$$，用他除以keep-prob参数，下面我解释一下为什么要这么做？方便起见，我假设第三隐层上有50个单元或50个神经元，在一维上$$a^3$$是50，我们通过因式分解将他拆分成50*m维的，保留和删除他们的概率分别是0.8和0.2，这意味着最后被删除或归零的单元平均均有10个，我们写出$$z^{[4]}$$公式，我们的预期是$$a^{[3]}$$减少0.2，也就是说$$a^{[3]}$$中有0.2的元素被归零。为了不影响$$z^{[4]}$$的期望值，我们用$$w^{[4]}a^{[3]}$$除以0.8，他将会修正或者弥补我们所需的那0.2，$$a^{[3]}的期望值不会变。划线部分就是所谓的dropout方法，它的功能时不论keep-prob的值是多少，0.8 0.9甚至是1，1的时候表示不存在dropout，因为他会保留所有节点。

![](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610944774265.png)

inverted dropout方法通过除以keep-prob，确保$$a^{[3]}$$的期望值不变。

事实证明，在测试阶段，当我们评估一个神经网络时，inverted dropout使测试阶段变得更容易，因为他的数据扩展问题变少，我们接下来会讲。

据我了解，目前实施dropout的最常用的方法就是inverted dropout，建议大家动手实践一下。dropout早期的迭代版本都没有除以keep-prob，所以在测试阶段，平均值会变得越来越复杂，不过那些版本已经不再使用了。

现在我们用的是d向量，我们会发现不同的训练样本，清除的隐藏单元也不同。实际上，如果你通过相同训练集多次传递数据，每次训练数据的梯度不同，则随即对不同的隐藏单元归零，有时却并非如此，比如需要将相同的隐藏单元归零。第一次迭代梯度下降时，把一些隐藏单元归零，第二次迭代梯度下降时，也就是第二次遍历训练集时，对不同类型的隐藏单元归零。向量d或者$$d^3$$用来决定第三层中哪些单元归零，无论用fp还是bp，这里我们只介绍了fp。

如何在测试阶段训练算法？在测试阶段，我们已经给出了x或是想预测的变量，用的是标准计数法。我们在测试阶段不使用dropout函数，尤其在下列情况：![1610945501869](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610945501869.png)

显然在测试阶段，我们并没有使用dropout，自然也就不用抛硬币来决定失活概率以及要消除哪些隐藏单元了。因为在测试阶段进行预测时，我们不希望输出结果是随机的。如果预测阶段应用dropout函数，预测会受到干扰。理论上，我们只需要多次运行预测处理过程，每一次不同的隐藏单元会被随机归零，预测处理遍历他们，但是计算效率低，得出的结果也几乎相同。

inverted dropout函数在除以keep-prob时可以记住上一步的操作，目的是确保即使在测试阶段不执行dropout来调整数值范围，激活函数的预期结果也不会发生变化，所以没必要在测试阶段额外添加尺度参数，这与训练阶段不同，这就是dropout，大家可以通过本周的编程练习来执行这个函数，亲身实践一下，为什么dropout会起作用呢？下节课我们将更加直观的了解dropout的具体功能。

***

④Dropout可以随即删除神经网络的神经单元，做法有点疯狂，他为什么可以通过正则化发挥这么大的作用呢？我们来更直观地理解一下。通过之前的学习我们知道，使用了dropout每次迭代之后，神经网络都会变得比以前更小。因此采用一个较小的神经网络和使用正则化的效果是一样的。第二个直观认识是我们从单个神经元入手：![1610955721197](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610955721197.png)

这个单元的工作就是输入并生成一些有意义的输出，通过dropout，该单元的输入几乎被消除。也就是说这个单元他不能依赖任何特征，因为特征都有可能被随机清除，或者说该单元的输入也有可能被随机清除。因此我们不愿意把一个赌注都放在一个节点上，不愿意给任何一个输入加上太多权重，因为他可能会被删除。

因此该单元将通过这种方式积极地传播开，并为单元的四个输入增加一点权重。通过传播所有权重，dropout将产生收缩权重的平方范数的效果。和我们之前讲过的L2正则化类似，实施dropout的结果是他会压缩权重，并完成一些预防过拟合的外层正则化。事实证明，dropout被正式地作为一种正则化的替代形式。L2对不同权重的衰减是不同的，他取决于倍增的激活函数的大小。总结一下dropout的效果类似于L2正则化。与L2正则化不同的是，被应用的方式不同，dropout也会有所不同，甚至更适用不同的输入范围。![1610956279443](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610956279443.png)

实施dropout的另一个细节是，其中一个要选择的参数是keep-prob，他代表每一层上保留单元的概率，所以不同层上的keep-prob也可以变化。![1610956306916](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610956306916.png)

$$w^{[2]}$$是最大的权重矩阵，因为$$w^{[2]}$$拥有最大参数集，即7*7。为了预防矩阵的过拟合，对于这一层，keep-prob的值相对来说应该较低。对于其它层，过拟合的程度可能没那么严重，他们的keep-prob就要高一些。这种操作有点像在处理L2正则化的正则化参数$$\lambda$$。

技术上讲，我们也可以对输入层应用dropout，我们有机会删除1个或多个输入特征，虽然现实中，我们通常不会这么做。keep-prob的值是1，是输入层非常常用的输入值。也可以用0.9，但是消除一半以上的输入特征是不太可能的，我们要谨记这个特征。

总结一下，如果你担心某些层比其它层更容易发生过拟合，可以把某些层的keep-prob值设置得比其他层更低。缺点是为了使用交叉验证，你要搜索更多的超级参数。另一种方案是在一些层应用dropout，而有些层不用dropout，应用dropout的层只含有一个超级参数，就是keep-prob。

结束前分享两个实施过程中的技巧，实施dropout在计算机视觉领域有很多成功的第一次，计算机视觉中的输入量非常大，输入了太多像素，以至于没有足够的数据。所以dropout在计算机视觉中应用得比较频繁。有些计算机视觉研究人员非常喜欢用他，几乎成了默认的选择。

但是要牢记一点，dropout是一种正则化方法，它有助于预防过拟合。因此，除非算法过拟合，不然我是不会使用dropout的，所以它在其他领域应用的比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情于dropout函数的原因。直观上，我认为不能概括其他学科。dropout的一大缺点就是代价函数J不再被明确定义，每次迭代都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难复查的。而定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义或者在某种程度上很难计算，所以我们失去了调测工具来绘制这样的图片。![1610957048828](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610957048828.png)

我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减，然后再打开dropout函数，在dropout过程中，代码并未引入bug，我觉得你也可以尝试其他方法，虽然我们并没有关于这些方法性能的数据统计。但是你可以把他们与dropout方法一起使用。

所以值得大家学习的正则化方法并不止这一个，接下来我们会介绍其他的正则化方法。

***

⑤数据扩增可以作为正则化方法使用，实际功能上也与正则化相似。![1610957438075](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610957438075.png)

还有另外一种常用的方法叫做early stopping，运行梯度下降时，我们可以绘制训练误差或只绘制代价函数J的优化过程，在训练集上用0-1记录分类误差次数呈单调下降趋势。因为在训练过程中，我们希望训练误差、代价函数J都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，他可以是验证集上的分类误差或验证集上的代价函数、逻辑损失和对数损失等。你会发现，验证集误差通常先会呈下降趋势，然后在某个节点处开始上升。

early stopping的作用，你会说神经网络在这个迭代过程中表现得很好了，我们在此停止训练吧，得到验证集误差，他是怎么发挥作用的？当你还未在神经网络上运行太多迭代过程的时候，参数w接近于0。因为随机初始化w值时，他的值可能都是较小的随机值，所以在你长期训练神经网络之前，w依然很小。在迭代过程和训练过程中，w的值会变得越来越大。比如在最右端w的值已经非常大了。

所以early stopping要做的就是在中间点停止迭代过程。我们得到一个w值中等大小的费罗贝尼乌斯范数。与L2正则化相似，选择w范数较小的神经网络，但愿你的神经网络过度拟合不严重。术语early stopping代表提早停止训练神经网络。

训练神经网络时，我有时会用到early stopping，但是他也有一个缺点，我们来了解一下。我认为机器过程包括几个步骤：其中一步是选择一个算法来优化代价函数J，我们有多种工具来解决这个问题，如梯度下降，后面我会介绍其他算法，如Momentum、RMSprop和Adam等等。但是优化代价函数J之后，我也不想发生过拟合，也有一些工具可以解决该问题，比如正则化、扩增数据等等。

在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。我发现，如果我们用一组工具优化代价函数J，机器学习就会变得更简单。在重点优化代价函数J时，你只需要留意w和b，J(w,b)的值越小越好，你只需要想办法减小这个值，其他的不用关注。然后预防过拟合还有其他任务，换句话说就是减少方差，这一步我们用另外一套工具来实现，这个原理有时候被称为正交化，思路是在一个时间做一个任务。

但是对于我来说early stopping的主要缺点是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为你现在不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方式同时解决两个问题，这样做的结果是我们要考虑的东西变得更复杂。如果不用early stopping，另一种方法就是L2正则化，训练神经网络的时间就可能很长，我发现，这导致超级参数搜索空间更容易分解，也更容易搜索。但是缺点在于，你必须尝试很多正则化参数$$\lambda$$的值，这也导致了搜索大量$$\lambda$$值的计算代价太高。

early stopping的优点是只运行一次梯度下降，你可以找到w的较小值、中间值和较大值，而无需尝试L2正则化超级参数$$\lambda$$的很多值。虽然L2正则化有缺点，可还是有很多人愿意用他，我个人更倾向于使用L2正则化，尝试许多不同的$$\lambda$$值。假设我们可以负担大量计算的代价，而是用early stopping也能得到相似结果，还不用尝试这么多$$\lambda$$值。![[1610959615057]()](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610959615057.png)

这节课我们讲了如何使用数据扩增以及如何使用early stopping降低神经网络中的方差或预防过拟合。下节课我们会讲一些配置优化问题的方法来提高训练速度。

***

# 编程作业第一周

⑥ipynb是ipython notebook的缩写，是jupyter可以识别的后缀， 其用来数据分析和画图非常方便的，所以有时在工作和学习中会遇到这样的文件。

在新建main函数的时候，我在main函数里面添加了网页上找的[源程序](https://blog.csdn.net/u013733326/article/details/79639509)， 结果报错：unable to open file: name = 'datasets/train_catvnoncat.h5，我找了一段时间的错误原因，最后找到原因说是需要[ 在ipynb的根目录下建立datasets文件夹](https://blog.csdn.net/will_ye/article/details/83346614)，我移动了几次datasets文件夹的位置，同时将main.py位置与datasets同级，如下：![1610970172847](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610970172847.png)

就运行成功了。

有一个问题为啥运行的是main(1)?

**这个问题终于搞定啦，太开心，因为移动文件夹导致的程序的运行工作路径改变，所以会出来main(1)，然后我们点击编辑配置环境，删除多余的main(1)和main(2)，再重新选择一下main的工作路径，在与程序中设置的路径要在同一级才可以。之前就是因为没设置对才导致运行失败的：![1610981526327](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610981526327.png)**

**据我个人的理解，工作路径也就是说在什么路径下面运行程序，所以确定好了才可以在lr_utils中选择相对路径，然后主程序才能正常运行。就是这样！**

<span style='color:red;background:背景颜色;font-size:文字大小;font-family:字体;'>改变颜色代码</span>

另外，print ("训练集的数量: m_train = " + str(m_train))，这句话中： +号是字符串连接操作， str函数是把数字转为字符串。 

 .shape[0]输出矩阵的行数,同理.shape[1]输出矩阵的列数。

 np.exp()函数是求$$e^{x}$$的值的函数。当np.exp()的参数传入的是一个向量时，其返回值是该向量内所以元素值分别进行$$e^{x}$$求值后的结果，所构成的一个列表返回给调用处。

预处理后的变量将会通过train_set_x_orig = np.array(train_dataset["train_set_x"] [:])命名为train_set_x。

 a[:] # 从左往右切取完整对象。

np.dot(a, b, out=None)  #该函数的作用是获取两个元素a,b的乘积.

squeeze()函数的功能是：从矩阵shape中，去掉维度为1的。例如一个矩阵是的shape是（5， 1），使用过这个函数后，结果为（5，）。

用渐变下降更新参数，目标是通过最小化成本函数J来学习w和b。

这篇文章讲得也不错，logistic回归的[代码流程](https://blog.csdn.net/hqh131360239/article/details/79075832)。

***

⑦归一化输入。训练神经网络，其中一个加速训练的方法就是归一化输入。假设我们有一个训练集，他有两个输入特征，所以输入特征x是二维的，数据集的散点图如下：![1610979048177](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610979048177.png)

归一化输入需要两个步骤，第一步是零均值化：
$$
\mu=\frac{1}{m}\sum_i^{m}x^{(i)} \ \ \ \ \ \ ①
$$

$$
x:=x-\mu    \ \ \ \ \ \ \ \ \ \ \ \ \ \ ②
$$

②的意思是移动训练集，知道他完成零均值化。

第二步是归一化方差，要注意，特征$$x_1$$的方差比特征$$x_2$$的方差要大得多：![1610979718095](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610979718095.png)

我们要做的就是给$$\sigma$$赋值：
$$
\sigma^2=\frac{1}{m}\sum_i^mx^{(i)^2}
$$
$$x^{(i)^2}$$是节点y的平方，$$\sigma^2$$是一个向量，他的每一个特征都有方差，注意我们已经完成零均值化。$$x^{(i)^2}-y^2$$就是方差，我们把所有数据都除以向量$$\sigma^2$$，图片最后变成这样：![1610980106229](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610980106229.png)

现在$$x_1$$和$$x_2$$的方差都等于1，提示一下，如果你用它来调整训练数据，那么用相同的$$\mu$$和$$\sigma^2$$来归一化测试集，尤其是，你不希望训练集和测试集的归一化有所不同。不论$$\mu$$的值是什么，也不论$$\sigma^2$$的值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估$$\mu$$和$$\sigma^2$$。因为我们希望不论是训练数据还是测试数据，都是通过相同的$$\mu$$和$$\sigma^2$$定义的相同数据转换。其中$$\mu$$和$$\sigma^2$$是由训练集数据计算得来的。

我们为什么要这样做呢？为什么我们想归一化输入特征，回想一下代价函数：![1610980535715](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610980535715.png)

如果我们使用非归一化的输入特征，代价函数将会像这样：![1610980578485](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610980578485.png)

这是一个非常细长狭窄的代价函数，我们要找的最小值应该在这里：![1610980618912](C:\Users\qiao\AppData\Roaming\Typora\typora-user-images\1610980618912.png)

但如果特征值在不同范围，假如特征$$x_1$$取值范围从1到1000，特征$$x_2$$的取值范围从0到1。

